{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0ee4fc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdataprep\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n",
      "File \u001b[1;32mD:\\py\\Lib\\site-packages\\seaborn\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelational\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[1;32mD:\\py\\Lib\\site-packages\\seaborn\\relational.py:17\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_oldcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     VectorPlotter,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     locator_to_legend_entries,\n\u001b[0;32m     13\u001b[0m     adjust_legend_subtitles,\n\u001b[0;32m     14\u001b[0m     _default_color,\n\u001b[0;32m     15\u001b[0m     _deprecate_ci,\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_statistics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EstimateAggregator\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxisgrid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FacetGrid, _facet_docs\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_docstrings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocstringComponents, _core_docs\n",
      "File \u001b[1;32mD:\\py\\Lib\\site-packages\\seaborn\\_statistics.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gaussian_kde\n\u001b[0;32m     32\u001b[0m     _no_scipy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\py\\Lib\\site-packages\\scipy\\stats\\__init__.py:608\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m \n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    607\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 608\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mD:\\py\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyVersion\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suppress_warnings\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _measurements\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, MapWrapper, _get_nan,\n\u001b[0;32m     42\u001b[0m                               rng_integers, _rename_parameter, _contains_nan)\n",
      "File \u001b[1;32mD:\\py\\Lib\\site-packages\\scipy\\spatial\\__init__.py:110\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m=============================================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mSpatial algorithms and data structures (:mod:`scipy.spatial`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m   QhullError\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_qhull\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mD:\\py\\Lib\\site-packages\\scipy\\spatial\\_kdtree.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright Anne M. Archibald 2008\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Released under the scipy license\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cKDTree, cKDTreeNode\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance_p\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRectangle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDTree\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminkowski_distance_p\u001b[39m(x, y, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[1;32m_ckdtree.pyx:12\u001b[0m, in \u001b[0;36minit scipy.spatial._ckdtree\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mD:\\py\\Lib\\site-packages\\scipy\\sparse\\__init__.py:287\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix_io\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# For backward compatibility with v0.19.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csgraph\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Deprecated namespaces, to be removed in v2.0.0\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    291\u001b[0m     base, bsr, compressed, construct, coo, csc, csr, data, dia, dok, extract,\n\u001b[0;32m    292\u001b[0m     lil, sparsetools, sputils\n\u001b[0;32m    293\u001b[0m )\n",
      "File \u001b[1;32mD:\\py\\Lib\\site-packages\\scipy\\sparse\\csgraph\\__init__.py:186\u001b[0m\n\u001b[0;32m    159\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnected_components\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    160\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaplacian\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    161\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortest_path\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsgraph_to_masked\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    183\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNegativeCycleError\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_laplacian\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m laplacian\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shortest_path\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    187\u001b[0m     shortest_path, floyd_warshall, dijkstra, bellman_ford, johnson,\n\u001b[0;32m    188\u001b[0m     NegativeCycleError\n\u001b[0;32m    189\u001b[0m )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traversal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    191\u001b[0m     breadth_first_order, depth_first_order, breadth_first_tree,\n\u001b[0;32m    192\u001b[0m     depth_first_tree, connected_components\n\u001b[0;32m    193\u001b[0m )\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_min_spanning_tree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minimum_spanning_tree\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataprep\n",
    "import sklearn\n",
    "import cartopy\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost\n",
    "import optuna\n",
    "import shap\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_predict\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81afbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Model selection and evaluation tools\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from tutorial9 import plot_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to clear the warnings from the notebook, usually we should leave this on\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Plot settings\n",
    "sns.set_context('notebook') # optimises figures for notebook display\n",
    "sns.set_style('ticks') # set default plot style\n",
    "colours = ['#4E79A7','#F28E2C','#E15759','#76B7B2','#59A14F', \n",
    "          '#EDC949','#AF7AA1','#FF9DA7','#9C755F','#BAB0AB']\n",
    "sns.set_palette(colours) # set custom color scheme\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e3991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('listings.csv')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa024a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['host_acceptance_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76fc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e984932",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['price'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bee354",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['price'] = data['price'].replace('[\\$,]', '', regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77340052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data['LogPrice'] = np.log(data['price'])\n",
    "\n",
    "index_train, index_test = train_test_split(data.index, train_size=0.7, random_state=21)\n",
    "\n",
    "train = data.loc[index_train, :].copy()\n",
    "test = data.loc[index_test, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0df39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Null_variable = ['description','neighbourhood_group_cleansed','bathrooms','calendar_updated']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc24036",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a59b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "Useless_variable = ['id','listing_url','scrape_id','last_scraped','picture_url','host_id','host_url','host_name','host_thumbnail_url','host_picture_url','calendar_last_scraped','host_verifications','license']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed121576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dabl import detect_types\n",
    "\n",
    "detected = detect_types(train)\n",
    "detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf9b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_features = detected[detected['useless'] == True].index.to_list()\n",
    "train[useless_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051bd627",
   "metadata": {},
   "source": [
    "#host_has_profile_pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f942fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['host_has_profile_pic'].value_counts()\n",
    "sns.boxplot(x = data['host_has_profile_pic'], y = data['LogPrice'], boxprops=dict(alpha=.9), palette='Blues')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e330d",
   "metadata": {},
   "source": [
    "bedroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926b570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['bedrooms'].notna() == True,['bedrooms','room_type','beds','bathrooms_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ea316",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['room_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03de431",
   "metadata": {},
   "source": [
    "amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b11a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['amenities'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f422d12",
   "metadata": {},
   "source": [
    "calculated_host_listings_count_shared_rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2dd2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['calculated_host_listings_count_shared_rooms'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts =data['calculated_host_listings_count_shared_rooms'].value_counts()\n",
    "for level in counts.index:\n",
    "    if counts[level] < 100:\n",
    "        train.loc[data['calculated_host_listings_count_shared_rooms'] == level, 'calculated_host_listings_count_shared_rooms'] = '2'\n",
    "        test.loc[data['calculated_host_listings_count_shared_rooms'] == level, 'calculated_host_listings_count_shared_rooms'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466b1e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['calculated_host_listings_count_shared_rooms'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ef4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['calculated_host_listings_count_shared_rooms'] = train['calculated_host_listings_count_shared_rooms'].astype(float)\n",
    "test['calculated_host_listings_count_shared_rooms'] = test['calculated_host_listings_count_shared_rooms'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = train['calculated_host_listings_count_shared_rooms'], y = data['LogPrice'], boxprops=dict(alpha=.9), palette='Blues')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8cf38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_useless_variable = ['bedrooms','amenities','host_has_profile_pic']\n",
    "All_deleted_variales = detected_useless_variable + Useless_variable + Null_variable\n",
    "train = train.drop(All_deleted_variales,axis = 1)\n",
    "test = test.drop(All_deleted_variales,axis = 1)\n",
    "data = data.drop(All_deleted_variales,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efece75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3187e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nominal_Variable = ['source','host_location','host_neighbourhood','property_type',\n",
    "                    'room_type','neighbourhood_cleansed','neighbourhood',\n",
    "                    'host_is_superhost','host_identity_verified','bathrooms_text',\n",
    "                   'has_availability','instant_bookable']\n",
    "Ordinal_Variable = ['host_response_time','review_scores_rating','review_scores_accuracy','review_scores_cleanliness',\n",
    "                   'review_scores_checkin','review_scores_communication','review_scores_location',\n",
    "                   'review_scores_value','reviews_per_month']\n",
    "Discrete_Variable = ['host_listings_count','host_total_listings_count','accommodates','beds','minimum_nights',\n",
    "                    'maximum_nights','minimum_minimum_nights','maximum_minimum_nights','minimum_maximum_nights',\n",
    "                    'maximum_maximum_nights','availability_30','availability_60','availability_90','availability_365',\n",
    "                    'number_of_reviews','number_of_reviews_ltm','number_of_reviews_l30d','calculated_host_listings_count',\n",
    "                    'calculated_host_listings_count_entire_homes','calculated_host_listings_count_private_rooms',\n",
    "                    'calculated_host_listings_count_shared_rooms']\n",
    "Continuous_Variable = ['host_response_rate','host_acceptance_rate','latitude','longitude','minimum_nights_avg_ntm',\n",
    "                       'maximum_nights_avg_ntm']\n",
    "Date_Variable = ['host_since','first_review','last_review']\n",
    "Text_Variable = ['name','neighborhood_overview','host_about']\n",
    "Response = ['LogPrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "\n",
    "median_log_price = train['LogPrice'].median()\n",
    "\n",
    "colors = ['red' if price > median_log_price else 'blue' for price in train['LogPrice']]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.Mercator())\n",
    "\n",
    "ax.scatter(train['longitude'], train['latitude'], c=colors, \n",
    "           edgecolors='none', alpha=0.3, transform=ccrs.PlateCarree())\n",
    "\n",
    "ax.add_feature(cfeature.STATES.with_scale('50m'))\n",
    "ax.add_feature(cfeature.COASTLINE.with_scale('50m'))\n",
    "\n",
    "ax.set_extent([150, 155, -35, -27], crs=ccrs.PlateCarree())\n",
    "\n",
    "ax.set_title('House Price in NSW (training data)', fontsize=17, y=1.01, fontweight='bold')\n",
    "ax.text(150.0, -33.86, 'Sydney', fontsize=8,transform=ccrs.Geodetic(), color='tab:red')\n",
    "plt.show()\n",
    "# Use median of log price to distinguish. Those above the median are in red, and those below the medinian are in blue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d49f667",
   "metadata": {},
   "source": [
    "# 1.Nominal Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc816e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['source'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572019ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = train['source'], y = train['LogPrice'], boxprops=dict(alpha=.9), palette='Blues')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded60cd2",
   "metadata": {},
   "source": [
    "### Therefore, we assume the previous scrape means the house has not been rented within 65 days and it has been already scraped before. And city scrape means the house for rent is newly listing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c24a587",
   "metadata": {},
   "source": [
    "### Then we choose dummy encoding to transfer two types of scrape into 0 and 1, zero is city scrape and 1 is previous scrape. the base type is city scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a022014",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'source'\n",
    "train_source_dummies = pd.get_dummies(train[variable],  prefix = variable, \n",
    "                         drop_first=True)\n",
    "\n",
    "test_source_dummies = pd.get_dummies(test[variable],  prefix = variable, \n",
    "                         drop_first=True)\n",
    "train_source_dummies = train_source_dummies.astype(float)\n",
    "test_source_dummies = test_source_dummies.astype(float)\n",
    "\n",
    "train = train.join(train_source_dummies)\n",
    "test = test.join(test_source_dummies)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea2c09",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1668bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['host_location','host_neighbourhood','neighbourhood_cleansed', 'neighbourhood']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6c05c7",
   "metadata": {},
   "source": [
    "### Because neighbourhood_cleansed is generated through latitude and longitude, it may be more accurate, host location shows the location of the host, not the location actual tenants live. Therefore, we use neighbourhood_cleansed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['neighbourhood_cleansed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4312f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "train['neighbourhood_cleansed'].value_counts().plot(kind='barh')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368cca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood_counts = data['neighbourhood_cleansed'].value_counts()\n",
    "\n",
    "neighbourhood_counts_df = pd.DataFrame(neighbourhood_counts).reset_index()\n",
    "neighbourhood_counts_df.columns = ['neighbourhood', 'count']\n",
    "\n",
    "sns.boxplot(data=neighbourhood_counts_df, y='count')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Neighbourhood')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d547078",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_count = np.median(neighbourhood_counts)\n",
    "print(median_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac78c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts =data['neighbourhood_cleansed'].value_counts()\n",
    "for level in counts.index:\n",
    "    if counts[level] < 385:\n",
    "        train.loc[data['neighbourhood_cleansed'] == level, 'neighbourhood_cleansed'] = 'Other'\n",
    "        test.loc[data['neighbourhood_cleansed'] == level, 'neighbourhood_cleansed'] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538105e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "train['neighbourhood_cleansed'].value_counts().plot(kind='barh')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train1, index_valid1 = train_test_split(train.index, train_size=0.7, random_state=21)\n",
    "\n",
    "train1 = train.loc[index_train1, :].copy()\n",
    "valid1 = train.loc[index_valid1, :].copy()\n",
    "\n",
    "y_train_1 = train1['LogPrice']\n",
    "y_valid_1 = valid1['LogPrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f225e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder #eliminate data leakage\n",
    "from category_encoders.glmm import GLMMEncoder\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "\n",
    "# This cell is a bit repetitive to keep the code simpler\n",
    "\n",
    "predictor = ['neighbourhood_cleansed']\n",
    "\n",
    "encoder = TargetEncoder().fit(train1[predictor], y_train_1) # we must use the training set for this\n",
    "train1['NeighborhoodTE'] = encoder.transform(train1[predictor])\n",
    "valid1['NeighborhoodTE'] = encoder.transform(valid1[predictor])\n",
    "\n",
    "encoder = LeaveOneOutEncoder().fit(train1[predictor], y_train_1) \n",
    "train1['NeighborhoodLeaveOneOut'] = encoder.transform(train1[predictor])\n",
    "valid1['NeighborhoodLeaveOneOut'] = encoder.transform(valid1[predictor])\n",
    "\n",
    "encoder =  GLMMEncoder().fit(train1[predictor], y_train_1)\n",
    "train1['NeighborhoodGLMM'] = encoder.transform(train1[predictor])\n",
    "valid1['NeighborhoodGLMM'] = encoder.transform(valid1[predictor])\n",
    "\n",
    "encoder = CatBoostEncoder().fit(train1[predictor], y_train_1) \n",
    "train1['NeighborhoodCatBoost'] = encoder.transform(train1[predictor])\n",
    "valid1['NeighborhoodCatBoost'] = encoder.transform(valid1[predictor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40dbda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "rows = ['Target encoder', 'Leave-one-out encoder', \n",
    "      'GLMM encoder', 'CatBoost encoder']\n",
    "\n",
    "columns=['RMSE', 'R-Squared', 'MAE']\n",
    "\n",
    "results = pd.DataFrame(0.0, columns=columns, index=rows)\n",
    "\n",
    "for i in range(len(rows)):\n",
    "\n",
    "    if i==0: \n",
    "        predictors = ['NeighborhoodTE'] \n",
    "    elif i==1:\n",
    "        predictors = ['NeighborhoodLeaveOneOut']\n",
    "    elif i==2: \n",
    "        predictors = ['NeighborhoodGLMM']\n",
    "    elif i==3: \n",
    "        predictors = ['NeighborhoodCatBoost']  \n",
    "    \n",
    "    X_train_1 =  train1[predictors]\n",
    "    X_valid_1 = valid1[predictors]\n",
    "    \n",
    "    # Fit model\n",
    "    ols = LinearRegression().fit(X_train_1, y_train_1)\n",
    "\n",
    "    # validation set predictions\n",
    "    y_pred_1 = ols.predict(X_valid_1)\n",
    "    results.iloc[i, 0] = np.sqrt(mean_squared_error(y_valid_1, y_pred_1))\n",
    "    results.iloc[i, 1] = r2_score(y_valid_1, y_pred_1)\n",
    "    results.iloc[i, 2] = mean_absolute_error(y_valid_1, y_pred_1)\n",
    "\n",
    "results.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06720500",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['property_type','room_type']].isna().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e623fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['property_type','room_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800272cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['property_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc295821",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['property_type'] = data['property_type'].apply(lambda x: x.split()[-1])\n",
    "data['property_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea200040",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['property_type'] = train['property_type'].apply(lambda x: x.split()[-1])\n",
    "test['property_type'] = test['property_type'].apply(lambda x: x.split()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48aa243",
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_counts =data['property_type'].value_counts()\n",
    "for level in pro_counts.index:\n",
    "    if pro_counts[level] < 40:\n",
    "        train.loc[data['property_type'] == level, 'property_type'] = 'Other'\n",
    "        test.loc[data['property_type'] == level, 'property_type'] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87baf9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,10))\n",
    "sns.boxplot(x = train['property_type'], y = train['LogPrice'], boxprops=dict(alpha=.9), palette='Blues')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea61af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'property_type'\n",
    "train_property_dummies = pd.get_dummies(train[variable], prefix = variable)\n",
    "\n",
    "test_property_dummies = pd.get_dummies(test[variable], prefix = variable)\n",
    "\n",
    "train_property_dummies = train_property_dummies.astype(float)\n",
    "test_property_dummies = test_property_dummies.astype(float)\n",
    "\n",
    "train = train.join(train_property_dummies)\n",
    "test = test.join(test_property_dummies)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dc5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_property_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38462880",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_property_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['room_type'].value_counts().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['room_type'].value_counts().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7762c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,10))\n",
    "sns.boxplot(x = train['room_type'], y = train['LogPrice'], boxprops=dict(alpha=.9), palette='Blues')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abcce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'room_type'\n",
    "train_room_dummies = pd.get_dummies(train[variable],  prefix = variable)\n",
    "\n",
    "test_room_dummies = pd.get_dummies(test[variable],  prefix = variable)\n",
    "\n",
    "train_room_dummies = train_room_dummies.astype(float)\n",
    "test_room_dummies = test_room_dummies.astype(float)\n",
    "\n",
    "train = train.join(train_room_dummies)\n",
    "test = test.join(test_room_dummies)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3afc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['host_is_superhost'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f568df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep.eda.missing import plot_missing\n",
    "plot_missing(train['host_is_superhost'],display  = ['Stats', 'Bar Chart'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['host_is_superhost'] = train['host_is_superhost'].fillna('NotEvaluate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b688f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['host_is_superhost'] = test['host_is_superhost'].fillna('NotEvaluate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03516a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'host_is_superhost'\n",
    "train_host_dummies = pd.get_dummies(train[variable],  prefix = variable, \n",
    "                         drop_first=True)\n",
    "\n",
    "test_host_dummies = pd.get_dummies(test[variable],  prefix = variable, \n",
    "                         drop_first=True)\n",
    "\n",
    "train_host_dummies = train_host_dummies.astype(float)\n",
    "test_host_dummies = test_host_dummies.astype(float)\n",
    "\n",
    "train = train.join(train_host_dummies)\n",
    "test = test.join(test_host_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea605d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep.eda.missing import plot_missing\n",
    "plot_missing(train['host_identity_verified'],display  = ['Stats', 'Bar Chart'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {\n",
    "        'f':0 , # Failed to pass verification\n",
    "        't':1, # verified\n",
    "}\n",
    "train['host_identity_verified'] = train['host_identity_verified'].replace(keys)\n",
    "test['host_identity_verified'] = test['host_identity_verified'].replace(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3af860",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['host_identity_verified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827f2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bathrooms_text'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f58e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_bathroom_nan = train.loc[train['bathrooms_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe8f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_bathroom_nan.iloc[:,15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84406e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bathrooms_text'] = train.apply(lambda row: str(row['accommodates'] / 2) + ' bathrooms' if pd.isna(row['bathrooms_text']) else row['bathrooms_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e85ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bathrooms_text'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8130b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_non_numeric(value):\n",
    "    if \"half\"in value.lower():\n",
    "        return 0.5 \n",
    "    else:\n",
    "        return float(value.split()[0]) \n",
    "\n",
    "train['bathrooms_text'] = train['bathrooms_text'].apply(replace_non_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6625d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bathrooms_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afc032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['bathrooms_text'] = test.apply(lambda row: str(row['accommodates'] / 2) + ' bathrooms' if pd.isna(row['bathrooms_text']) else row['bathrooms_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b743537",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['bathrooms_text'] = test['bathrooms_text'].apply(replace_non_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed60ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['has_availability'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e00f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {\n",
    "        'f':0 , # cannot automatically book\n",
    "        't':1, # can automatically book\n",
    "}\n",
    "train['has_availability'] = train['has_availability'].replace(keys)\n",
    "test['has_availability'] = test['has_availability'].replace(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['instant_bookable'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {\n",
    "        'f':0 , # not available\n",
    "        't':1, # available\n",
    "}\n",
    "train['instant_bookable'] = train['instant_bookable'].replace(keys)\n",
    "test['instant_bookable'] = test['instant_bookable'].replace(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5bc0e",
   "metadata": {},
   "source": [
    "# 2. Ordinal Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ordinal_Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f84785d",
   "metadata": {},
   "source": [
    "## 2.1 host_response_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd8f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['host_response_time','host_response_rate','host_acceptance_rate']].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f3ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['host_response_time'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed3db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['host_response_time'] = train['host_response_time'].fillna('No yet reviewed')\n",
    "test['host_response_time'] = test['host_response_time'].fillna('No yet reviewed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = train['host_response_time'], y = train['LogPrice'], boxprops=dict(alpha=.9), palette='Blues')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c5031",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {\n",
    "        'No yet reviewed': 0,\n",
    "        'within an hour': 1,\n",
    "        'within a few hours': 2,\n",
    "        'within a day': 3,\n",
    "        'a few days or more': 4, \n",
    "}\n",
    "\n",
    "variable = 'host_response_time'\n",
    "train[variable] = train[variable].replace(keys)\n",
    "test[variable] = test[variable].replace(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aec099",
   "metadata": {},
   "source": [
    "## 2.2 reviews_per_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb78dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['reviews_per_month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['reviews_per_month'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ca6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews(x):\n",
    "    if x > 5:\n",
    "        return 4\n",
    "    elif x > 3:\n",
    "        return 3\n",
    "    elif x > 1:\n",
    "        return 2\n",
    "    elif x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "train['reviews_per_month'] = train['reviews_per_month'].apply(reviews)\n",
    "test['reviews_per_month'] = test['reviews_per_month'].apply(reviews)\n",
    "train['reviews_per_month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef1aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = train['reviews_per_month'], y = train['LogPrice'], boxprops=dict(alpha=.9), palette='Blues')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8211d2cb",
   "metadata": {},
   "source": [
    "## 2.3 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7da649",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = ['review_scores_rating','review_scores_accuracy','review_scores_cleanliness',\n",
    "                   'review_scores_checkin','review_scores_communication','review_scores_location',\n",
    "                   'review_scores_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86049c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "corr = train[score].corr()\n",
    "n_var = len(corr) # Number of variables\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.imshow(corr, cmap = 'viridis',vmin=-1, vmax=1)\n",
    "\n",
    "plt.xticks(range(n_var), corr.columns,rotation=90)\n",
    "plt.yticks(range(n_var), corr.columns)\n",
    "\n",
    "for i in range(n_var):\n",
    "    for j in range(n_var):\n",
    "        #              format to 2 decimal places  ,  horizonta/vertical align center  \n",
    "        plt.text(i, j, '{:.2f}'.format(corr.iloc[i, j]), ha=\"center\", va=\"center\", color=\"k\", fontsize = 22)\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Correlations')\n",
    "plt.savefig('correlation.png')\n",
    "# All variables of score-related are highly correlated. \n",
    "# Hence I choose the 'review_scores_rating' as the baseline since it is the most representative variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a5b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['review_scores_rating','first_review','last_review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed550506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_standardized(x):\n",
    "    if x > 4.5:\n",
    "        return 5\n",
    "    elif x > 3.5:\n",
    "        return 4\n",
    "    elif x > 2.5:\n",
    "        return 3\n",
    "    elif x > 1.5:\n",
    "        return 2\n",
    "    elif x > 0.5:\n",
    "        return 1\n",
    "    elif x > 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 #-1 means not yet reviewed\n",
    "    \n",
    "for col in score:\n",
    "    train[col] = train[col].apply(rate_standardized)\n",
    "    test[col] = test[col].apply(rate_standardized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review_scores_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f801b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = train['review_scores_rating'], y = train['LogPrice'], boxprops=dict(alpha=.9), palette='Blues')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d788dd",
   "metadata": {},
   "source": [
    "# 3. Discrete Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a122f4",
   "metadata": {},
   "source": [
    "## 3.1 Discrete variable correlation analysis and data frequency presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ef683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep.eda import plot_correlation\n",
    "\n",
    "plot_correlation(train[Discrete_Variable + Response], 'LogPrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f86be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep.eda import plot\n",
    "plot(train[Discrete_Variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))  \n",
    "corr = train[Discrete_Variable].corr()\n",
    "\n",
    "\n",
    "plt.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1)  \n",
    "\n",
    "\n",
    "plt.xticks(range(len(corr)), corr.columns, rotation=90)  \n",
    "plt.yticks(range(len(corr)), corr.columns)\n",
    "\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Correlations')\n",
    "plt.savefig('correlation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaa94da",
   "metadata": {},
   "source": [
    "## 3.2 Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['host_listings_count','host_total_listings_count','accommodates','beds','minimum_nights',\n",
    "                    'maximum_nights','minimum_minimum_nights','maximum_minimum_nights','minimum_maximum_nights',\n",
    "                    'maximum_maximum_nights','availability_30','availability_60','availability_90','availability_365',\n",
    "                    'number_of_reviews','number_of_reviews_ltm','number_of_reviews_l30d','calculated_host_listings_count',\n",
    "                    'calculated_host_listings_count_entire_homes','calculated_host_listings_count_private_rooms',\n",
    "                    'calculated_host_listings_count_shared_rooms']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e387fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep.eda.missing import plot_missing\n",
    "plot_missing(train['beds'],display  = ['Stats', 'Bar Chart'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e0c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_beds_train = train['beds'].isna()\n",
    "missing_data_train = train[missing_beds_train][['accommodates']]\n",
    "train.loc[missing_beds_train, 'beds'] = missing_data_train['accommodates']\n",
    "\n",
    "missing_beds_test = test['beds'].isna()\n",
    "missing_data_test = test[missing_beds_test][['accommodates']]\n",
    "test.loc[missing_beds_test, 'beds'] = missing_data_test['accommodates']\n",
    "\n",
    "# Result\n",
    "print(\"Number of missing values in 'beds' column in train set:\", train['beds'].isna().sum())\n",
    "print(\"Number of missing values in 'beds' column in test set:\", test['beds'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb465e",
   "metadata": {},
   "source": [
    "## 3.3 listings_count_stats and Total_Listing_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_count_stats = train['host_listings_count'].describe()\n",
    "\n",
    "print(listings_count_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49758d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_host = train['host_listings_count'].quantile(0.25)\n",
    "Q3_host = train['host_listings_count'].quantile(0.75)\n",
    "IQR_host = Q3_host - Q1_host\n",
    "\n",
    "lower_limit_host = Q1_host - 1.5 * IQR_host\n",
    "upper_limit_host = Q3_host + 1.5 * IQR_host\n",
    "\n",
    "train['host_listings_count'] = train['host_listings_count'].apply(lambda x: upper_limit_host if x > upper_limit_host else (lower_limit_host if x < lower_limit_host else x))\n",
    "\n",
    "test['host_listings_count'] = test['host_listings_count'].apply(lambda x: upper_limit_host if x > upper_limit_host else (lower_limit_host if x < lower_limit_host else x))\n",
    "\n",
    "print(train['host_listings_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7deb3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_listings_count= train['host_total_listings_count'].describe()\n",
    "\n",
    "print(Total_listings_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_total = train['host_total_listings_count'].quantile(0.25)\n",
    "Q3_total = train['host_total_listings_count'].quantile(0.75)\n",
    "IQR_total = Q3_total - Q1_total\n",
    "\n",
    "lower_limit_total = Q1_total - 1.5 * IQR_total\n",
    "upper_limit_total = Q3_total + 1.5 * IQR_total\n",
    "\n",
    "train['host_total_listings_count'] = train['host_total_listings_count'].apply(lambda x: upper_limit_total if x > upper_limit_total else (lower_limit_total if x < lower_limit_total else x))\n",
    "\n",
    "test['host_total_listings_count'] = test['host_total_listings_count'].apply(lambda x: upper_limit_total if x > upper_limit_total else (lower_limit_total if x < lower_limit_total else x))\n",
    "\n",
    "print(train['host_total_listings_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a900c",
   "metadata": {},
   "source": [
    "## 3.4 Beds and accommodates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['beds'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5caa868",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(train['beds'], vert=False)\n",
    "plt.xlabel('beds_number')\n",
    "plt.title('Boxplot of beds_number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_beds = train['beds'].quantile(0.25)\n",
    "Q3_beds = train['beds'].quantile(0.75)\n",
    "IQR_beds = Q3_beds - Q1_beds\n",
    "\n",
    "lower_limit_beds = Q1_beds - 1.5 * IQR_beds\n",
    "upper_limit_beds = Q3_beds + 1.5 * IQR_beds\n",
    "\n",
    "train['beds'] = train['beds'].apply(lambda x: upper_limit_beds if x > upper_limit_beds else (lower_limit_beds if x < lower_limit_beds else x))\n",
    "test['beds'] = test['beds'].apply(lambda x: upper_limit_beds if x > upper_limit_beds else (lower_limit_beds if x < lower_limit_beds else x))\n",
    "\n",
    "trimmed_counts = train['beds'].value_counts()\n",
    "print(\"Number of occurrences of each value after processing:\")\n",
    "print(trimmed_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bac078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "test['accommodates'] = test['accommodates'].apply(lambda x: float(re.findall(r'\\d+\\.?\\d*', str(x))[0]) if pd.notnull(x) else np.nan)\n",
    "\n",
    "large_rooms_accommodates_test = test[test['accommodates'] > 10]\n",
    "\n",
    "print(\"Data for rooms with more than 10 people in the test set:\")\n",
    "print(large_rooms_accommodates_test[['beds', 'accommodates']])\n",
    "\n",
    "excess_accommodates_test = large_rooms_accommodates_test[large_rooms_accommodates_test['accommodates'] > 2 * large_rooms_accommodates_test['beds']]\n",
    "\n",
    "excess_accommodates_test['accommodates'] = excess_accommodates_test['beds'] * 2\n",
    "\n",
    "replacement_count_test = excess_accommodates_test.shape[0]\n",
    "\n",
    "print(\"Replaced accommodates in the test set:\", replacement_count_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c530813a",
   "metadata": {},
   "source": [
    "## 3.5 minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "\n",
    "columns_1125 = ['minimum_nights', 'maximum_nights', \n",
    "           'minimum_minimum_nights', 'maximum_minimum_nights', \n",
    "           'minimum_maximum_nights', 'maximum_maximum_nights']\n",
    "\n",
    "for i, column in enumerate(columns_1125):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    axs[row, col].hist(train[column], bins=30, color='skyblue', edgecolor='black')\n",
    "    axs[row, col].set_title(f'Histogram of {column}')\n",
    "    axs[row, col].set_xlabel(column)\n",
    "    axs[row, col].set_ylabel('Frequency')\n",
    "    axs[row, col].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ec62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['minimum_nights', 'maximum_nights', 'minimum_minimum_nights', \n",
    "                    'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights']\n",
    "\n",
    "for column in selected_columns:\n",
    "    train.loc[train[column] > 1125, column] = 1125\n",
    "\n",
    "for column in selected_columns:\n",
    "    test.loc[test[column] > 1125, column] = 1125\n",
    "\n",
    "def classify_nights(x):\n",
    "    if x <= 30:\n",
    "        return 0\n",
    "    elif x <= 60:\n",
    "        return 1\n",
    "    elif x <= 90:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "def classify_max_nights(x):\n",
    "    if x >= 1125:\n",
    "        return 1\n",
    "    elif x >= 730:\n",
    "        return 2\n",
    "    elif x >= 365:\n",
    "        return 3\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "for column in ['minimum_minimum_nights', 'minimum_nights', 'maximum_minimum_nights']:\n",
    "    train[f'{column}_class'] = train[column].apply(classify_nights)\n",
    "    test[f'{column}_class'] = test[column].apply(classify_nights)\n",
    "\n",
    "for column in ['minimum_maximum_nights', 'maximum_maximum_nights', 'maximum_nights']:\n",
    "    train[f'{column}_class'] = train[column].apply(classify_max_nights)  \n",
    "    test[f'{column}_class'] = test[column].apply(classify_max_nights)\n",
    "\n",
    "for column in ['minimum_minimum_nights', 'minimum_nights', 'maximum_minimum_nights']:\n",
    "    print(f\"Train {column} class counts:\")\n",
    "    print(train[f'{column}_class'].value_counts())\n",
    "    print(\"\")\n",
    "\n",
    "for column in ['minimum_maximum_nights', 'maximum_maximum_nights', 'maximum_nights']:\n",
    "    print(f\"Train {column} class counts:\")\n",
    "    print(train[f'{column}_class'].value_counts())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32aa745",
   "metadata": {},
   "source": [
    "## 3.5availability_30', 'availability_60', 'availability_90', 'availability_365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns_availability = ['availability_30', 'availability_60', 'availability_90', 'availability_365']\n",
    "\n",
    "for column in selected_columns_availability:\n",
    "    counts = train[column].value_counts()\n",
    "    print(f\"{column} number of occurrences:\")\n",
    "    print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns_availability = ['availability_30', 'availability_60', 'availability_90', 'availability_365']\n",
    "\n",
    "for column in selected_columns_availability:\n",
    "    train[column] = train[column].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "for column in selected_columns_availability:\n",
    "    test[column] = test[column].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "for column in selected_columns_availability:\n",
    "    print(f\"{column} number of occurrences after processing in train set:\")\n",
    "    print(train[column].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d58a5",
   "metadata": {},
   "source": [
    "## 3.6 number_of_reviews','number_of_reviews_ltm','number_of_reviews_l30d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227339c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, variable in enumerate(variables, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.boxplot(train[variable], vert=False)\n",
    "    plt.xlabel(variable)\n",
    "    plt.title(f'Boxplot of {variable}')\n",
    "    \n",
    "\n",
    "    median = train[variable].median()\n",
    "    mean = train[variable].mean()\n",
    "    plt.axvline(median, color='r', linestyle='--', label=f'Median: {median:.2f}')\n",
    "    plt.axvline(mean, color='g', linestyle='--', label=f'Mean: {mean:.2f}')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_reviews_ltm(num_reviews_ltm, median_reviews_ltm, mean_reviews_ltm):\n",
    "    if num_reviews_ltm < median_reviews_ltm:\n",
    "        return 1\n",
    "    elif num_reviews_ltm == median_reviews_ltm:\n",
    "        return 2\n",
    "    elif num_reviews_ltm < mean_reviews_ltm:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "#  'number_of_reviews_ltm'\n",
    "median_reviews_ltm = train['number_of_reviews_ltm'].median()\n",
    "mean_reviews_ltm = train['number_of_reviews_ltm'].mean()\n",
    "\n",
    "train['number_of_reviews_ltm'] = train['number_of_reviews_ltm'].apply(classify_reviews_ltm,\n",
    "                                                                     args=(median_reviews_ltm, mean_reviews_ltm))\n",
    "\n",
    "print(\"Review Category Counts for 'number_of_reviews_ltm' in the train set:\")\n",
    "print(train['number_of_reviews_ltm'].value_counts())\n",
    "\n",
    "test['number_of_reviews_ltm'] = test['number_of_reviews_ltm'].apply(classify_reviews_ltm,\n",
    "                                                                   args=(median_reviews_ltm, mean_reviews_ltm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb02789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_reviews(num_reviews, median_reviews, mean_reviews):\n",
    "    if num_reviews < median_reviews:\n",
    "        return 1\n",
    "    elif num_reviews == median_reviews:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "#  'number_of_reviews_ltm'\n",
    "median_reviews_ltm = train['number_of_reviews_ltm'].median()\n",
    "mean_reviews_ltm = train['number_of_reviews_ltm'].mean()\n",
    "\n",
    "train['number_of_reviews_ltm'] = train['number_of_reviews_ltm'].apply(classify_reviews,\n",
    "                                                                     args=(median_reviews_ltm, mean_reviews_ltm))\n",
    "\n",
    "print(\"Review Category Counts for 'number_of_reviews_ltm' in the train set:\")\n",
    "print(train['number_of_reviews_ltm'].value_counts())\n",
    "\n",
    "test['number_of_reviews_ltm'] = test['number_of_reviews_ltm'].apply(classify_reviews,\n",
    "                                                                   args=(median_reviews_ltm, mean_reviews_ltm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Need to judge how to classify by images (special to the first two because the box-and-line diagrams are not )\n",
    "\n",
    "plt.hist(train['number_of_reviews_l30d'], bins=30, color='skyblue', edgecolor='black')\n",
    "\n",
    "plt.title('Distribution of number_of_reviews_l30d')\n",
    "plt.xlabel('Number of Reviews (Last 30 Days)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65706a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_reviews(x):\n",
    "    if x <= 2:\n",
    "        return 0\n",
    "    elif x <= 4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "train['review_level_l30d'] = train['number_of_reviews_l30d'].apply(classify_reviews)\n",
    "test['review_level_l30d'] = test['number_of_reviews_l30d'].apply(classify_reviews)\n",
    "review_level_counts = train['review_level_l30d'].value_counts()\n",
    "\n",
    "print(review_level_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"review_level_l30d\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a96ae",
   "metadata": {},
   "source": [
    "## 3.7 calculated_host_listings_count', 'calculated_host_listings_count_entire_homes',calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa48b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['calculated_host_listings_count'] = train['calculated_host_listings_count'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "\n",
    "test['calculated_host_listings_count'] = test['calculated_host_listings_count'].apply(lambda x: 0 if x == 1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_value(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    elif x == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def apply_classification(df):\n",
    "    df['entire_homes_category'] = df['calculated_host_listings_count_entire_homes'].apply(classify_value)\n",
    "    df['private_rooms_category'] = df['calculated_host_listings_count_private_rooms'].apply(classify_value)\n",
    "    df['shared_rooms_category'] = df['calculated_host_listings_count_shared_rooms'].apply(classify_value)\n",
    "    return df\n",
    "\n",
    "train = apply_classification(train)\n",
    "\n",
    "test = apply_classification(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16adb439",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_homes_counts_train = train['entire_homes_category'].value_counts()\n",
    "\n",
    "private_rooms_counts_train = train['private_rooms_category'].value_counts()\n",
    "shared_rooms_counts_train = train['shared_rooms_category'].value_counts()\n",
    "calculated_host_counts_train = train['calculated_host_listings_count'].value_counts()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "entire_homes_counts_train.plot(kind='bar', color='blue')\n",
    "plt.title('Entire Homes Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "private_rooms_counts_train.plot(kind='bar', color='green')\n",
    "plt.title('Private Rooms Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "shared_rooms_counts_train.plot(kind='bar', color='orange')\n",
    "plt.title('Shared Rooms Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "calculated_host_counts_train.plot(kind='bar', color='red')\n",
    "plt.title('Calculated Host Listings Count Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf40d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = train['calculated_host_listings_count_shared_rooms'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca775fc",
   "metadata": {},
   "source": [
    "# 4 Continuous Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420af74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Continuous_Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da778f7",
   "metadata": {},
   "source": [
    "## 4.1 Some EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7102485",
   "metadata": {},
   "source": [
    "### 4.1.1 host_response_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73053ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['host_response_rate'] = train['host_response_rate'].str.rstrip('%').astype(float)\n",
    "test['host_response_rate'] = test['host_response_rate'].str.rstrip('%').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37960ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_response_rate_train = train['host_response_rate'].mean()\n",
    "host_response_rate_test = test['host_response_rate'].mean()\n",
    "train['host_response_rate'] = train['host_response_rate'].fillna(host_response_rate_train)\n",
    "test['host_response_rate'] = test['host_response_rate'].fillna(host_response_rate_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca242c",
   "metadata": {},
   "source": [
    "### 4.1.2 host_acceptance_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['host_acceptance_rate'] = train['host_acceptance_rate'].str.rstrip('%').astype(float)\n",
    "test['host_acceptance_rate'] = test['host_acceptance_rate'].str.rstrip('%').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad323138",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_acceptance_rate_train = train['host_acceptance_rate'].mean()\n",
    "host_acceptance_rate_test = test['host_acceptance_rate'].mean()\n",
    "train['host_acceptance_rate'] = train['host_acceptance_rate'].fillna(host_acceptance_rate_train)\n",
    "test['host_acceptance_rate'] = test['host_acceptance_rate'].fillna(host_acceptance_rate_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3283c5",
   "metadata": {},
   "source": [
    "### 4.1.3 maximum_nights_avg_ntm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb31b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['maximum_nights_avg_ntm'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff377c5",
   "metadata": {},
   "source": [
    "### 4.1.4 minimum_nights_avg_ntm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed34255",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['minimum_nights_avg_ntm'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10)) \n",
    "corr = train[Continuous_Variable].corr()\n",
    "\n",
    "\n",
    "plt.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1)  \n",
    "\n",
    "plt.xticks(range(len(corr)), corr.columns, rotation=90)  \n",
    "plt.yticks(range(len(corr)), corr.columns)\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Correlations')\n",
    "plt.savefig('correlation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff362d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep.eda import plot_correlation\n",
    "\n",
    "plot_correlation(train[Continuous_Variable + Response], 'LogPrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbabb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_train = train.sort_values(by='maximum_nights_avg_ntm', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f093827",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_train['maximum_nights_avg_ntm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19428785",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['maximum_nights_avg_ntm'] > 20000, 'maximum_nights_avg_ntm'] = 1125\n",
    "test.loc[test['maximum_nights_avg_ntm'] > 20000, 'maximum_nights_avg_ntm'] = 1125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c2d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep.eda import plot\n",
    "\n",
    "plot(train[Continuous_Variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a07bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum_nights(x):\n",
    "    if x >= 90:\n",
    "        return 'Longer than 3 months'\n",
    "    elif x >= 30:\n",
    "        return 'Longer than 1 month but less than 3 months'\n",
    "    else:\n",
    "        return 'Less than 1 month'\n",
    "train['minimum_nights_avg_ntm'] = train['minimum_nights_avg_ntm'].apply(minimum_nights)\n",
    "test['minimum_nights_avg_ntm'] = test['minimum_nights_avg_ntm'].apply(minimum_nights)\n",
    "train['minimum_nights_avg_ntm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['minimum_nights_avg_ntm'] = train['minimum_nights_avg_ntm'].replace({'Less than 1 month': 1, 'Longer than 1 month but less than 3 months': 2, 'Longer than 3 months': 3})\n",
    "test['minimum_nights_avg_ntm'] = test['minimum_nights_avg_ntm'].replace({'Less than 1 month': 1, 'Longer than 1 month but less than 3 months': 2, 'Longer than 3 months': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e862f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_nights(x):\n",
    "    if x >= 1125:\n",
    "        return 'Longer than 3 years and 1 month'\n",
    "    elif x >= 730:\n",
    "        return 'Longer than 2 years but less than 3 years and 1 month'\n",
    "    elif x >= 365:\n",
    "        return 'Longer than 1 year but less than 2 years'\n",
    "    else:\n",
    "        return 'Less than 1 year'\n",
    "train['maximum_nights_avg_ntm'] = train['maximum_nights_avg_ntm'].apply(maximum_nights)\n",
    "test['maximum_nights_avg_ntm'] = test['maximum_nights_avg_ntm'].apply(maximum_nights)\n",
    "train['maximum_nights_avg_ntm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6effd330",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['maximum_nights_avg_ntm'] = train['maximum_nights_avg_ntm'].replace({'Less than 1 year': 1, 'Longer than 1 year but less than 2 years': 2, 'Longer than 2 years but less than 3 years and 1 month': 3, 'Longer than 3 years and 1 month': 4})\n",
    "test['maximum_nights_avg_ntm'] = test['maximum_nights_avg_ntm'].replace({'Less than 1 year': 1, 'Longer than 1 year but less than 2 years': 2, 'Longer than 2 years but less than 3 years and 1 month': 3, 'Longer than 3 years and 1 month': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10525379",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = train['minimum_nights_avg_ntm'], y = train['LogPrice'], boxprops=dict(alpha=.9), palette='Blues')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 10))\n",
    "sns.boxplot(x = train['maximum_nights_avg_ntm'], y = train['LogPrice'], boxprops=dict(alpha=.9), palette='Blues')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6532e289",
   "metadata": {},
   "source": [
    "## 4.2 Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a975e91",
   "metadata": {},
   "source": [
    "### 4.2.1 host_response_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['host_response_rate'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ad6e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = train['host_response_rate'], y = train['LogPrice'], \n",
    "            lowess=True, line_kws={'color':'black', 'alpha':0.6})\n",
    "\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d5ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Loghost_response_rate'] = np.log(train['host_response_rate'])\n",
    "test['Loghost_response_rate'] = np.log(test['host_response_rate'])\n",
    "\n",
    "index_train2, index_valid2 = train_test_split(train.index, train_size=0.7, random_state=21)\n",
    "\n",
    "train2 = train.loc[index_train2, :].copy()\n",
    "valid2 = train.loc[index_valid2, :].copy()\n",
    "\n",
    "train2['Loghost_response_rate'].fillna(0, inplace=True)\n",
    "valid2['Loghost_response_rate'].fillna(0, inplace=True)\n",
    "train2['Loghost_response_rate'] = train2['Loghost_response_rate'].replace(-np.inf, 0)\n",
    "valid2['Loghost_response_rate'] = valid2['Loghost_response_rate'].replace(-np.inf, 0)\n",
    "\n",
    "y_train_2 = train2['LogPrice']\n",
    "y_valid_2 = valid2['LogPrice']\n",
    "\n",
    "\n",
    "sns.regplot(x = train['Loghost_response_rate'], y = train['LogPrice'], \n",
    "            lowess=True,  line_kws={'color':'black', 'alpha':0.6})\n",
    "\n",
    "sns.despine()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "yj = PowerTransformer(method='yeo-johnson')\n",
    "train2['host_response_rateYJ']  = yj.fit_transform(train2[['host_response_rate']])\n",
    "valid2['host_response_rateYJ'] = yj.transform(valid2[['host_response_rate']])\n",
    "\n",
    "bc = PowerTransformer(method='box-cox')\n",
    "train2['host_response_rateBC']  = yj.fit_transform(train2[['host_response_rate']])\n",
    "valid2['host_response_rateBC'] = yj.transform(valid2[['host_response_rate']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aefef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train2, 'host_response_rateYJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf3b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train2, 'host_response_rateBC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd4896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise table\n",
    "rows = ['No transformation', 'Log transformation', 'Yeo-Johnson transformation','Box-cox transformation']\n",
    "columns=['RMSE', 'R-Squared', 'MAE']\n",
    "results = pd.DataFrame(0.0, columns=columns, index=rows)\n",
    "\n",
    "for i in range(len(rows)):\n",
    "\n",
    "    # Design matrices\n",
    "    if i==0:\n",
    "        predictor = ['host_response_rate']\n",
    "    elif i==1:\n",
    "        predictor = ['Loghost_response_rate']\n",
    "    elif i==2:\n",
    "        predictor = ['host_response_rateYJ']\n",
    "    else: \n",
    "        predictor = ['host_response_rateBC']\n",
    "    \n",
    "    X_train2 =  train2[predictor]\n",
    "    X_valid2 = valid2[predictor]\n",
    "    \n",
    "    # Fit model\n",
    "    ols = LinearRegression().fit(X_train2, y_train_2)\n",
    "\n",
    "    # Validation set predictions\n",
    "    y_pred2 = ols.predict(X_valid2)\n",
    "    \n",
    "    # Validation metrics\n",
    "    results.iloc[i, 0] = np.sqrt(mean_squared_error(y_valid_2, y_pred2))\n",
    "    results.iloc[i, 1] = r2_score(y_valid_2, y_pred2)\n",
    "    results.iloc[i, 2] = mean_absolute_error(y_valid_2, y_pred2)\n",
    "\n",
    "results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5256da",
   "metadata": {},
   "source": [
    "### 4.2.2 host_acceptance_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ef5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['host_acceptance_rate'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = train['host_acceptance_rate'], y = train['LogPrice'], \n",
    "            lowess=True, line_kws={'color':'black', 'alpha':0.6})\n",
    "\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34871b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Loghost_acceptance_rate'] = np.log(train['host_acceptance_rate'])\n",
    "test['Loghost_acceptance_rate'] = np.log(test['host_acceptance_rate'])\n",
    "\n",
    "index_train2, index_valid2 = train_test_split(train.index, train_size=0.7, random_state=21)\n",
    "\n",
    "train2 = train.loc[index_train2, :].copy()\n",
    "valid2 = train.loc[index_valid2, :].copy()\n",
    "\n",
    "train2['Loghost_acceptance_rate'].fillna(0, inplace=True)\n",
    "valid2['Loghost_acceptance_rate'].fillna(0, inplace=True)\n",
    "train2['Loghost_acceptance_rate'] = train2['Loghost_acceptance_rate'].replace(-np.inf, 0)\n",
    "valid2['Loghost_acceptance_rate'] = valid2['Loghost_acceptance_rate'].replace(-np.inf, 0)\n",
    "\n",
    "y_train_2 = train2['LogPrice']\n",
    "y_valid_2 = valid2['LogPrice']\n",
    "\n",
    "\n",
    "sns.regplot(x = train['Loghost_acceptance_rate'], y = train['LogPrice'], \n",
    "            lowess=True,  line_kws={'color':'black', 'alpha':0.6})\n",
    "\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef4da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "yj = PowerTransformer(method='yeo-johnson')\n",
    "train2['host_acceptance_rateYJ']  = yj.fit_transform(train2[['host_acceptance_rate']])\n",
    "valid2['host_acceptance_rateYJ'] = yj.transform(valid2[['host_acceptance_rate']])\n",
    "\n",
    "bc = PowerTransformer(method='box-cox')\n",
    "train2['host_acceptance_rateBC']  = yj.fit_transform(train2[['host_acceptance_rate']])\n",
    "valid2['host_acceptance_rateBC'] = yj.transform(valid2[['host_acceptance_rate']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f86d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train2, 'host_acceptance_rateYJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise table\n",
    "rows = ['No transformation', 'Log transformation', 'Yeo-Johnson transformation','Box-cox transformation']\n",
    "columns=['RMSE', 'R-Squared', 'MAE']\n",
    "results = pd.DataFrame(0.0, columns=columns, index=rows)\n",
    "\n",
    "for i in range(len(rows)):\n",
    "\n",
    "    # Design matrices\n",
    "    if i==0:\n",
    "        predictor = ['host_acceptance_rate']\n",
    "    elif i==1:\n",
    "        predictor = ['Loghost_acceptance_rate']\n",
    "    elif i==2:\n",
    "        predictor = ['host_acceptance_rateYJ']\n",
    "    else: \n",
    "        predictor = ['host_acceptance_rateBC']\n",
    "    \n",
    "    X_train2 =  train2[predictor]\n",
    "    X_valid2 = valid2[predictor]\n",
    "    \n",
    "    # Fit model\n",
    "    ols = LinearRegression().fit(X_train2, y_train_2)\n",
    "\n",
    "    # Validation set predictions\n",
    "    y_pred2 = ols.predict(X_valid2)\n",
    "    \n",
    "    # Validation metrics\n",
    "    results.iloc[i, 0] = np.sqrt(mean_squared_error(y_valid_2, y_pred2))\n",
    "    results.iloc[i, 1] = r2_score(y_valid_2, y_pred2)\n",
    "    results.iloc[i, 2] = mean_absolute_error(y_valid_2, y_pred2)\n",
    "\n",
    "results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c16da6",
   "metadata": {},
   "source": [
    "### 4.2.3 Interaction between response and accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af20a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['host_response&accept_rate'] = train['host_response_rate'] * train['host_acceptance_rate']/100\n",
    "test['host_response&accept_rate'] = test['host_response_rate'] * test['host_acceptance_rate']/100\n",
    "train['host_response&accept_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdba8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = train['host_response&accept_rate'], y = train['LogPrice'], \n",
    "            lowess=True, line_kws={'color':'black', 'alpha':0.6})\n",
    "\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d524924",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Loghost_response&accept_rate'] = np.log(train['host_response&accept_rate'])\n",
    "test['Loghost_response&accept_rate'] = np.log(test['host_response&accept_rate'])\n",
    "\n",
    "index_train2, index_valid2 = train_test_split(train.index, train_size=0.7, random_state=21)\n",
    "\n",
    "train2 = train.loc[index_train2, :].copy()\n",
    "valid2 = train.loc[index_valid2, :].copy()\n",
    "\n",
    "train2['Loghost_response&accept_rate'].fillna(0, inplace=True)\n",
    "valid2['Loghost_response&accept_rate'].fillna(0, inplace=True)\n",
    "train2['Loghost_response&accept_rate'] = train2['Loghost_response&accept_rate'].replace(-np.inf, 0)\n",
    "valid2['Loghost_response&accept_rate'] = valid2['Loghost_response&accept_rate'].replace(-np.inf, 0)\n",
    "\n",
    "y_train_2 = train2['LogPrice']\n",
    "y_valid_2 = valid2['LogPrice']\n",
    "\n",
    "\n",
    "sns.regplot(x = train['Loghost_response&accept_rate'], y = train['LogPrice'], \n",
    "            lowess=True,  line_kws={'color':'black', 'alpha':0.6})\n",
    "\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "yj = PowerTransformer(method='yeo-johnson')\n",
    "train2['host_response&accept_rateYJ']  = yj.fit_transform(train2[['host_response&accept_rate']])\n",
    "valid2['host_response&accept_rateYJ'] = yj.transform(valid2[['host_response&accept_rate']])\n",
    "\n",
    "bc = PowerTransformer(method='box-cox')\n",
    "train2['host_response&accept_rateBC']  = yj.fit_transform(train2[['host_response&accept_rate']])\n",
    "valid2['host_response&accept_rateBC'] = yj.transform(valid2[['host_response&accept_rate']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac57882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise table\n",
    "rows = ['No transformation', 'Log transformation', 'Yeo-Johnson transformation','Box-cox transformation']\n",
    "columns=['RMSE', 'R-Squared', 'MAE']\n",
    "results = pd.DataFrame(0.0, columns=columns, index=rows)\n",
    "\n",
    "for i in range(len(rows)):\n",
    "\n",
    "    # Design matrices\n",
    "    if i==0:\n",
    "        predictor = ['host_response&accept_rate']\n",
    "    elif i==1:\n",
    "        predictor = ['Loghost_response&accept_rate']\n",
    "    elif i==2:\n",
    "        predictor = ['host_response&accept_rateYJ']\n",
    "    else: \n",
    "        predictor = ['host_response&accept_rateBC']\n",
    "    \n",
    "    X_train2 =  train2[predictor]\n",
    "    X_valid2 = valid2[predictor]\n",
    "    \n",
    "    # Fit model\n",
    "    ols = LinearRegression().fit(X_train2, y_train_2)\n",
    "\n",
    "    # Validation set predictions\n",
    "    y_pred2 = ols.predict(X_valid2)\n",
    "    \n",
    "    # Validation metrics\n",
    "    results.iloc[i, 0] = np.sqrt(mean_squared_error(y_valid_2, y_pred2))\n",
    "    results.iloc[i, 1] = r2_score(y_valid_2, y_pred2)\n",
    "    results.iloc[i, 2] = mean_absolute_error(y_valid_2, y_pred2)\n",
    "\n",
    "results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad1b26b",
   "metadata": {},
   "source": [
    "## 4.3 Longitude and Latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Longitude_Latitude'] = (train['longitude'] * train['latitude']).abs()\n",
    "test['Longitude_Latitude'] = (test['longitude'] * test['latitude']).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f68133",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Longitude_Latitude'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e269419",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y = train['Longitude_Latitude'], boxprops=dict(alpha=.9), palette='Blues')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3767ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['LogLongitude_Latitude'] = np.log(train['Longitude_Latitude'])\n",
    "test['LogLongitude_Latitude'] = np.log(test['Longitude_Latitude'])\n",
    "\n",
    "index_train3, index_valid3 = train_test_split(train.index, train_size=0.7, random_state=21)\n",
    "\n",
    "train3 = train.loc[index_train3, :].copy()\n",
    "valid3 = train.loc[index_valid3, :].copy()\n",
    "\n",
    "y_train_3 = train3['LogPrice']\n",
    "y_valid_3 = valid3['LogPrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18816902",
   "metadata": {},
   "outputs": [],
   "source": [
    "yj = PowerTransformer(method='yeo-johnson')\n",
    "train3['Longitude_LatitudeYJ']  = yj.fit_transform(train3[['Longitude_Latitude']])\n",
    "valid3['Longitude_LatitudeYJ'] = yj.transform(valid3[['Longitude_Latitude']])\n",
    "\n",
    "bc = PowerTransformer(method='box-cox')\n",
    "train3['Longitude_LatitudeBC']  = yj.fit_transform(train3[['Longitude_Latitude']])\n",
    "valid3['Longitude_LatitudeBC'] = yj.transform(valid3[['Longitude_Latitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac47381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise table\n",
    "rows = ['No transformation', 'Log transformation', 'Yeo-Johnson transformation','Box-cox transformation']\n",
    "columns=['RMSE', 'R-Squared', 'MAE']\n",
    "results = pd.DataFrame(0.0, columns=columns, index=rows)\n",
    "\n",
    "for i in range(len(rows)):\n",
    "\n",
    "    # Design matrices\n",
    "    if i==0:\n",
    "        predictor = ['Longitude_Latitude']\n",
    "    elif i==1:\n",
    "        predictor = ['LogLongitude_Latitude']\n",
    "    elif i==2:\n",
    "        predictor = ['Longitude_LatitudeYJ']\n",
    "    else: \n",
    "        predictor = ['Longitude_LatitudeBC']\n",
    "    \n",
    "    X_train3 =  train3[predictor]\n",
    "    X_valid3 = valid3[predictor]\n",
    "    \n",
    "    # Fit model\n",
    "    ols = LinearRegression().fit(X_train3, y_train_3)\n",
    "\n",
    "    # Validation set predictions\n",
    "    y_pred3 = ols.predict(X_valid3)\n",
    "    \n",
    "    # Validation metrics\n",
    "    results.iloc[i, 0] = np.sqrt(mean_squared_error(y_valid_3, y_pred3))\n",
    "    results.iloc[i, 1] = r2_score(y_valid_3, y_pred3)\n",
    "    results.iloc[i, 2] = mean_absolute_error(y_valid_3, y_pred3)\n",
    "\n",
    "results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814d761",
   "metadata": {},
   "source": [
    "# 5. Date Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['host_since'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cffd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['host_since'] = pd.to_datetime(data['host_since'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['host_since'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b40ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_day_to_now = pd.to_datetime('2023-12-31') - pd.to_datetime(train['host_since'])\n",
    "train['days_to_now'] = tr_day_to_now.dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5406dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_day_to_now = pd.to_datetime('2023-12-31') - pd.to_datetime(test['host_since'])\n",
    "test['days_to_now'] = te_day_to_now.dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['days_to_now'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9204051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['first_review'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8714e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['first_review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abf285",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['last_review'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a98e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the 'days'\n",
    "tr_day_reviews = pd.to_datetime(train['last_review']) - pd.to_datetime(train['first_review'])\n",
    "tr_day_reviews = tr_day_reviews.apply(lambda x: str(x.days) if not pd.isna(x) else np.nan)\n",
    "\n",
    "# replace nan to -1 as 'Notreviewed'\n",
    "train['day_reviews'] = tr_day_reviews.fillna(-1)\n",
    "\n",
    "train['day_reviews'] = train['day_reviews'].astype(float)\n",
    "train['day_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e5b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the 'days'\n",
    "te_day_reviews = pd.to_datetime(test['last_review']) - pd.to_datetime(test['first_review'])\n",
    "te_day_reviews = te_day_reviews.apply(lambda x: str(x.days) if not pd.isna(x) else np.nan)\n",
    "\n",
    "# replace nan to -1 as'NotReviewed'\n",
    "test['day_reviews'] = te_day_reviews.fillna(-1)\n",
    "test['day_reviews'] = test['day_reviews'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30943735",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['day_reviews'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbac1a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['day_reviews'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b023b",
   "metadata": {},
   "source": [
    "# 6. Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac25c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "New_variable=['review_scores_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "plt.figure(figsize=(12, 10))  \n",
    "Selected_discrete_variable=['host_total_listings_count','accommodates','minimum_nights','minimum_maximum_nights' ,'availability_60','number_of_reviews','calculated_host_listings_count']\n",
    "corr = train[Selected_discrete_variable + Continuous_Variable + New_variable + Response].corr()\n",
    "\n",
    "\n",
    "plt.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1) \n",
    "\n",
    "\n",
    "plt.xticks(range(len(corr)), corr.columns, rotation=90)  \n",
    "plt.yticks(range(len(corr)), corr.columns)\n",
    "\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Correlations')\n",
    "plt.savefig('correlation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_variable = train[Selected_discrete_variable + Continuous_Variable + New_variable + Response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d4c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(select_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d2490",
   "metadata": {},
   "source": [
    "### The latitude and longtude may affect different areas on the selected variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740d558",
   "metadata": {},
   "source": [
    "# 7. Text Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d3417",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[Text_Variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423692c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[Text_Variable].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ffc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['neighborhood_overview'] =train['neighborhood_overview'].fillna('NAN')\n",
    "test['neighborhood_overview'] =test['neighborhood_overview'].fillna('NAN')\n",
    "\n",
    "train['host_about'] =train['host_about'].fillna('NAN')\n",
    "test['host_about'] =test['host_about'].fillna('NAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3aa692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ec6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95196818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def tokenizer(text):\n",
    "    \n",
    "    # Replace emojis with special tokens\n",
    "    text = re.sub(r'[]', ' xxheart ', text)\n",
    "    text = re.sub(r'[]', ' xxsmile ', text) \n",
    "    text = re.sub(r'[]', ' xxsadface ', text) \n",
    "    text = re.sub(r'[]', ' xxlolface ', text) \n",
    "    \n",
    "    # Replace urls with a special token\n",
    "    text = re.sub('https?\\S+', 'xxurl', text) \n",
    "    text = re.sub('\\S*www[.]\\S+', 'xxurl', text)\n",
    "\n",
    "    # Replace emoticons with special tokens\n",
    "    text = re.sub(r'<3', 'xxheart', text) \n",
    "    text = re.sub(r'&lt;3', 'xxheart', text)\n",
    "    \n",
    "    eyes = r'[8:=;]'\n",
    "    nose = r'[\\'`-]*'\n",
    "    text = re.sub(eyes + nose + r'[)D]+', 'xxsmile', text)\n",
    "    text = re.sub(eyes + nose + r'[(]+', 'xxsadface', text)\n",
    "    text = re.sub(eyes + nose + r'p+', 'xxlolface', text)\n",
    "    text = re.sub(eyes + nose + r'[|/\\\\]+', 'xxneutralface', text)\n",
    "\n",
    "    # Creating a special token for hashtags\n",
    "    text = re.sub(r'#(\\S+)', 'xxhashtag ' + r'\\1', text) # hashtag\n",
    "    \n",
    "    # Stripping repeated whitespace\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for token in doc: \n",
    "        \n",
    "        word = token.lemma_.lower()\n",
    "        \n",
    "        if not token.is_stop:\n",
    "            \n",
    "            if word == '!': \n",
    "                tokens.append('!') \n",
    "                            \n",
    "                \n",
    "            elif (not token.is_punct) and word != '':\n",
    "                tokens.append(word)\n",
    "                \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9db3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_neigh = train['neighborhood_overview'].apply(tokenizer)\n",
    "\n",
    "print(tokens_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38368c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=identity_tokenizer,\n",
    "                        max_features =15,max_df =0.9,\n",
    "                        stop_words='english',lowercase=False,ngram_range = (2,2))\n",
    "\n",
    "corpus_neigh = train['neighborhood_overview'].apply(tokenizer) \n",
    "test_cor_neigh = test['neighborhood_overview'].apply(tokenizer) \n",
    "\n",
    "tfidf.fit(corpus_neigh)\n",
    "\n",
    "train_text_neigh = tfidf.transform(corpus_neigh).todense() \n",
    "test_text_neigh = tfidf.transform(test_cor_neigh).todense() \n",
    "name_list_neigh = tfidf.get_feature_names_out()\n",
    "\n",
    "train_text_neigh = pd.DataFrame(train_text_neigh,columns = name_list_neigh) \n",
    "test_text_neigh = pd.DataFrame(test_text_neigh,columns = name_list_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758bff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_neigh.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc441ce",
   "metadata": {},
   "source": [
    "# 8. Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1d8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_price = test[test['price']==100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefe8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_price[['price','accommodates','bathrooms_text','neighbourhood_cleansed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ca384",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_max = test['LogPrice'].idxmax()\n",
    "index_of_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea33853",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(index_of_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84772440",
   "metadata": {},
   "source": [
    "# 9.0 Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e1af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['LogPrice']\n",
    "\n",
    "encoder = CatBoostEncoder().fit(train['neighbourhood_cleansed'], y_train) \n",
    "train['neighbourhood_cleansed_cat'] = encoder.transform(train['neighbourhood_cleansed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe2516",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['LogPrice']\n",
    "\n",
    "encoder = CatBoostEncoder().fit(test['neighbourhood_cleansed'], y_test) \n",
    "test['neighbourhood_cleansed_cat'] = encoder.transform(test['neighbourhood_cleansed']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08cfcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "yj = PowerTransformer(method='yeo-johnson')\n",
    "train['host_response_rate_yj']  = yj.fit_transform(train[['host_response_rate']])\n",
    "test['host_response_rate_yj'] = yj.transform(test[['host_response_rate']])\n",
    "\n",
    "\n",
    "train['host_acceptance_rate_yj']  = yj.fit_transform(train[['host_acceptance_rate']])\n",
    "test['host_acceptance_rate_yj'] = yj.transform(test[['host_acceptance_rate']])\n",
    "\n",
    "\n",
    "train['host_response&accept_rate_yj']  = yj.fit_transform(train[['host_response&accept_rate']])\n",
    "test['host_response&accept_rate_yj'] = yj.transform(test[['host_response&accept_rate']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['neighborhood_overview','source','host_since','host_location','host_is_superhost','host_neighbourhood','neighbourhood_cleansed','property_type','room_type',\n",
    "                    'Loghost_response&accept_rate', 'LogLongitude_Latitude','neighbourhood',\n",
    "                    'first_review','last_review','host_response_rate','host_acceptance_rate',\n",
    "                    'Loghost_response_rate','Loghost_acceptance_rate','latitude','longitude',\n",
    "                    'host_response_rate_yj','host_acceptance_rate_yj','host_response&accept_rate',\n",
    "                    'name','host_about'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08842ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(columns=['neighborhood_overview','source','host_since','host_location','host_is_superhost','host_neighbourhood','neighbourhood_cleansed','property_type','room_type',\n",
    "                    'Loghost_response&accept_rate', 'LogLongitude_Latitude','neighbourhood',\n",
    "                    'first_review','last_review','host_response_rate','host_acceptance_rate',\n",
    "                    'Loghost_response_rate','Loghost_acceptance_rate','latitude','longitude',\n",
    "                    'host_response_rate_yj','host_acceptance_rate_yj','host_response&accept_rate',\n",
    "                    'name','host_about'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ea6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "train_text_neigh.reset_index(drop=True, inplace=True)\n",
    "train = pd.concat([train, train_text_neigh], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe39db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.reset_index(drop=True, inplace=True)\n",
    "test_text_neigh.reset_index(drop=True, inplace=True)\n",
    "test = pd.concat([test, test_text_neigh], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130de8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train, index_valid = train_test_split(train.index, train_size=0.7, random_state=21)\n",
    "\n",
    "sub_train = train.loc[index_train, :].copy()\n",
    "sub_valid = train.loc[index_valid, :].copy()\n",
    "\n",
    "y_sub_train = sub_train['LogPrice']\n",
    "y_sub_valid = sub_valid['LogPrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sub_train = sub_train.drop(columns=['LogPrice','price'])\n",
    "x_sub_valid = sub_valid.drop(columns=['LogPrice','price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d24ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sub_train.drop(columns=['/><br />the','< br','br />-','br /><br'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb4586",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sub_valid.drop(columns=['/><br />the','< br','br />-','br /><br'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78bef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['/><br />the','< br','br />-','br /><br'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b61a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(columns=['/><br />the','< br','br />-','br /><br'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f51f8ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f22520",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e306150",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sub_train[\"Longitude_Latitude\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1a978c",
   "metadata": {},
   "source": [
    "# 10.0 Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ba881",
   "metadata": {},
   "source": [
    "## 10.1 Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9eeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "ols=LinearRegression()\n",
    "ols.fit(x_sub_train,y_sub_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412942fe",
   "metadata": {},
   "source": [
    "## 10.2 Linear regression with ridge regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "%time\n",
    "alphas=list(np.logspace(-15,15,151,base=2))\n",
    "ridge=RidgeCV(alphas=alphas, cv = 5)\n",
    "ridge.fit(x_sub_train,y_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff33134",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd2058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial4 import coefplot\n",
    "coefplot(ridge, x_sub_train.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0976ea",
   "metadata": {},
   "source": [
    "## 10.3 Linear regression with lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "%time\n",
    "alphas=list(np.logspace(-15,15,151,base=2))\n",
    "lasso=LassoCV(alphas = alphas, cv=5)\n",
    "lasso.fit(x_sub_train,y_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6bc189",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9441ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial4 import coefplot\n",
    "coefplot(lasso, x_sub_train.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e91318",
   "metadata": {},
   "source": [
    "## 10.4 Linear regression with Elastic net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d33ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "elastic=ElasticNetCV(l1_ratio=[0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99], cv=5)\n",
    "elastic.fit(x_sub_train,y_sub_train)\n",
    "print(elastic.l1_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd43aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial4 import coefplot\n",
    "coefplot(elastic, x_sub_train.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42341f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['RMSE', 'R-Squared', 'MAE']\n",
    "rows=['Linear Regression', 'OLS $\\ell_1$','OLS $\\ell_2$','OLS Elastic net']\n",
    "\n",
    "results =pd.DataFrame(0.0, columns=columns, index=rows)\n",
    "\n",
    "methods = [ols, lasso, ridge, elastic]\n",
    "\n",
    "y_pred = np.zeros((len(y_sub_valid), 4))\n",
    "residual = np.zeros((len(y_sub_train), 4))\n",
    "avgexp_res = np.zeros((1, 4))\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    \n",
    "    residual[:,i] = y_sub_train - method.predict(x_sub_train)\n",
    "    avgexp_res[:,i] = np.mean(np.exp(residual[:,i]))\n",
    "    y_pred[:, i] = np.exp(method.predict(x_sub_valid)) * avgexp_res[:,i]\n",
    "    \n",
    "for i in range(len(rows)):\n",
    "    results.iloc[i, 0] = np.sqrt(mean_squared_error(np.exp(y_sub_valid), y_pred[:, i]))\n",
    "    results.iloc[i, 1] = r2_score(np.exp(y_sub_valid), y_pred[:, i])\n",
    "    results.iloc[i, 2] = mean_absolute_error(np.exp(y_sub_valid), y_pred[:, i])\n",
    "\n",
    "results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf87e4b",
   "metadata": {},
   "source": [
    "# 11.0 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f393b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=5,n_repeats=10,random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = KNeighborsRegressor(metric='mahalanobis', metric_params={'V': x_sub_train.cov()}) \n",
    "\n",
    "param_grid = {'n_neighbors': np.arange(1,51),}\n",
    "\n",
    "knn =  GridSearchCV(model, param_grid, cv = rkf,  scoring = 'neg_mean_squared_error', verbose=1)\n",
    "knn.fit(x_sub_train, y_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f092012",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10695cc",
   "metadata": {},
   "source": [
    " # 12.0 Generalised Additive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac58b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SplineTransformer\n",
    "from pygam import LinearGAM, s, f, l, te\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e79c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sub_train = np.exp(y_sub_train)\n",
    "y_sub_valid = np.exp(y_sub_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f61b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "gam = LinearGAM(s(0) + s(1) + s(2) + l(3) + s(4) + s(5) + s(6) + s(7) + s(8) + s(9) + s(10) + \n",
    "                      s(11) + s(12) + s(13) + s(14) + l(15) + l(16) + l(17) + l(18) + l(19) + l(20) + \n",
    "                      l(21) + l(22) + l(23) + l(24) + l(25) + l(26) + l(27) + l(28) + l(29) + l(30) + \n",
    "                      s(31) + l(32) + l(33) + l(34) + l(35) + l(36) +l(37)+ l(38) + l(39) + l(40) + \n",
    "                      l(41) + l(42) + l(43) + l(44) + l(45) + l(46) + l(47) + l(48) + l(49) + \n",
    "                      l(50) + l(51) + l(52) + l(53) + l(54) + l(55) + l(56) + l(57) + l(58) + \n",
    "                      l(59) + l(60) + l(61) + l(62) + l(63) + l(64) + l(65) + l(66) + l(67) + \n",
    "                      l(68) + l(69) + l(70) + l(71) + l(72) + l(73) + l(74) + s(75) + s(76) + \n",
    "                      s(77) + s(78) + s(79) + l(80) + l(81) + l(82) + l(83) + l(84) + l(85) + \n",
    "                      l(86) + l(87) + l(88))\n",
    "                \n",
    "gam.fit(x_sub_train, y_sub_train)\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lambdas = [trial.suggest_loguniform(f'lambda_{j}', 1e-4, 10000.0) for j in range(89)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = LinearGAM(s(0) + s(1) + s(2) + l(3) + s(4) + s(5) + s(6) + s(7) + s(8) + s(9) + s(10) + \n",
    "                      s(11) + s(12) + s(13) + s(14) + l(15) + l(16) + l(17) + l(18) + l(19) + l(20) + \n",
    "                      l(21) + l(22) + l(23) + l(24) + l(25) + l(26) + l(27) + l(28) + l(29) + l(30) + \n",
    "                      s(31) + l(32) + l(33) + l(34) + l(35) + l(36) +l(37)+ l(38) + l(39) + l(40) + \n",
    "                      l(41) + l(42) + l(43) + l(44) + l(45) + l(46) + l(47) + l(48) + l(49) + \n",
    "                      l(50) + l(51) + l(52) + l(53) + l(54) + l(55) + l(56) + l(57) + l(58) + \n",
    "                      l(59) + l(60) + l(61) + l(62) + l(63) + l(64) + l(65) + l(66) + l(67) + \n",
    "                      l(68) + l(69) + l(70) + l(71) + l(72) + l(73) + l(74) + s(75) + s(76) + \n",
    "                      s(77) + s(78) + s(79) + l(80) + l(81) + l(82) + l(83) + l(84) + l(85) + \n",
    "                      l(86) + l(87) + l(88),lam=lambdas)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model.fit(x_sub_train, y_sub_train)\n",
    "    return model.statistics_['GCV']\n",
    "\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=10000, timeout=3000)\n",
    "\n",
    "lambdas = list(study.best_params.values())\n",
    "\n",
    "\n",
    "\n",
    "gam_optimized = LinearGAM(s(0) + s(1) + s(2) + l(3) + s(4) + s(5) + s(6) + s(7) + s(8) + s(9) + s(10) + \n",
    "                      s(11) + s(12) + s(13) + s(14) + l(15) + l(16) + l(17) + l(18) + l(19) + l(20) + \n",
    "                      l(21) + l(22) + l(23) + l(24) + l(25) + l(26) + l(27) + l(28) + l(29) + l(30) + \n",
    "                      s(31) + l(32) + l(33) + l(34) + l(35) + l(36) +l(37)+ l(38) + l(39) + l(40) + \n",
    "                      l(41) + l(42) + l(43) + l(44) + l(45) + l(46) + l(47) + l(48) + l(49) + \n",
    "                      l(50) + l(51) + l(52) + l(53) + l(54) + l(55) + l(56) + l(57) + l(58) + \n",
    "                      l(59) + l(60) + l(61) + l(62) + l(63) + l(64) + l(65) + l(66) + l(67) + \n",
    "                      l(68) + l(69) + l(70) + l(71) + l(72) + l(73) + l(74) + s(75) + s(76) + \n",
    "                      s(77) + s(78) + s(79) + l(80) + l(81) + l(82) + l(83) + l(84) + l(85) + \n",
    "                      l(86) + l(87) + l(88),lam=lambdas)\n",
    "\n",
    "\n",
    "gam_optimized.fit(x_sub_train, y_sub_train)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y_sub_valid):\n",
    "    y_pred = model.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y_sub_valid, y_pred))\n",
    "    r2 = r2_score(y_sub_valid, y_pred)\n",
    "    mae = mean_absolute_error(y_sub_valid, y_pred)\n",
    "    return rmse, r2, mae\n",
    "\n",
    "rmse_gam, r2_gam, mae_gam = evaluate_model(gam_optimized, x_sub_valid, y_sub_valid)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['GAM'],\n",
    "    'RMSE': [rmse_gam,],\n",
    "    'R-Squared': [r2_gam],\n",
    "    'MAE': [mae_gam]\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea90d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b832f63",
   "metadata": {},
   "source": [
    "# 13.0 Tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c2a7ea",
   "metadata": {},
   "source": [
    "# 13.1 Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0200802",
   "metadata": {},
   "source": [
    "## 13.1.1 Growing the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30954366",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = x_sub_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d40e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "reg = DecisionTreeRegressor(criterion='squared_error', max_leaf_nodes=10)\n",
    "\n",
    "reg.fit(x_sub_train, y_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "dot_data = export_graphviz(reg, out_file=None , impurity=False, feature_names = feature_names, \n",
    "                           rounded=True) \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab51d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(reg, feature_names = feature_names,  impurity=False,\n",
    "          rounded=True, filled = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0706260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtreeviz\n",
    "viz = dtreeviz.model(reg,  x_sub_train, y_sub_train,target_name = 'LogPrice', feature_names = feature_names)  \n",
    "               \n",
    "viz.view(scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90a91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = DecisionTreeRegressor(criterion='squared_error', max_leaf_nodes=10)\n",
    "reg.fit(x_sub_train, y_sub_train)\n",
    "\n",
    "dot_data = export_graphviz(reg, out_file=None , impurity=False, feature_names = feature_names, rounded=True) \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b70be",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = dtreeviz.model(reg,  x_sub_train, y_sub_train, feature_names = feature_names)  \n",
    "              \n",
    "viz.view(scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc3c4c",
   "metadata": {},
   "source": [
    "## 13.1.2 Cost-complexity pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902633aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor(criterion='squared_error', min_samples_leaf=10)\n",
    "\n",
    "path = model.cost_complexity_pruning_path(x_sub_train, y_sub_train)\n",
    "\n",
    "alphas = path.ccp_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "impurities = path.impurities\n",
    "impurities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8aa0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(alphas))\n",
    "print(alphas[:10])\n",
    "\n",
    "plt.plot(alphas)\n",
    "#plt.ylabel('alpha')\n",
    "\n",
    "\n",
    "plt.plot(impurities)\n",
    "plt.legend(['ccp_alphas','impurities'] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea279a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "'''\n",
    "ccp_alphas: also called effective alphas, measures the strenght of link of a node to the tree.  \n",
    "Nodes with the smallest effective alpha have the weakest link and are pruned first.\n",
    "'''\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a8550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "model = DecisionTreeRegressor(criterion='squared_error', min_samples_leaf=5)\n",
    "\n",
    "search_space = {\n",
    "    'ccp_alpha': alphas,\n",
    "}\n",
    "\n",
    "tree_search = RandomizedSearchCV(model, search_space, cv = 5)\n",
    "tree_search.fit(x_sub_train, y_sub_train)\n",
    "tree = tree_search.best_estimator_\n",
    "\n",
    "print('Best parameters found by grid search:', tree_search.best_params_, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(reg, out_file=None , impurity=False, feature_names = feature_names,\n",
    "                           rounded=True) \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render('Optimal Tree') # saves tree to a file\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cacf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes=tree.tree_.node_count\n",
    "print('Number of leaves: ', n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856eb44",
   "metadata": {},
   "source": [
    "## 13.2 Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bag = BaggingRegressor(DecisionTreeRegressor(criterion='squared_error'), n_estimators=1000, random_state=42)\n",
    "bag.fit(x_sub_train, y_sub_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277e187",
   "metadata": {},
   "source": [
    "## 13.3 Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79419d2",
   "metadata": {},
   "source": [
    "### 13.3.1 Fitting a random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e47c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf =  RandomForestRegressor(n_estimators=100,  criterion='squared_error',  max_features = 20, min_samples_leaf= 1)\n",
    "\n",
    "rf.fit(x_sub_train, y_sub_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14eeaca",
   "metadata": {},
   "source": [
    "### 13.3.2 Hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "#suppress log messages\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    criterion = trial.suggest_categorical('criterion', ['squared_error', 'absolute_error'])\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    max_features = trial.suggest_int('max_features', 1, 20)\n",
    "    \n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators = 100,  \n",
    "                                  criterion = criterion,\n",
    "                                  max_features = max_features, \n",
    "                                  min_samples_leaf= min_samples_leaf,\n",
    "                                  random_state = 1)\n",
    "    \n",
    "    scores = cross_val_score(model, x_sub_train, y_sub_train, cv = 5, scoring = 'r2')\n",
    "    loss = 1 - np.mean(scores)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "sampler = TPESampler(seed=42) \n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study.optimize(objective, n_trials = 100, timeout = 3000) \n",
    "\n",
    "# the timeout should be as high as possible in a real application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79849a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert output to a dataframe.\n",
    "study_df=study.trials_dataframe()\n",
    "trials_df=study_df.drop(columns=['datetime_start', 'datetime_complete','duration' ])\n",
    "print('Size of output:', trials_df.shape)\n",
    "display('Trial info: ', trials_df.head())\n",
    "display('Best trial info: ', trials_df[trials_df.value == trials_df.value.min()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2b9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = study.best_params\n",
    "print(type(params)) #dictionary\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d54f689",
   "metadata": {},
   "outputs": [],
   "source": [
    " # **params \"unpacks\" the dictionary and passes the items as inputs to the random forest class\n",
    "    \n",
    "rf = RandomForestRegressor(n_estimators=1000, **params) \n",
    "rf.fit(x_sub_train, y_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee02806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial9 import plot_feature_importance\n",
    "plot_feature_importance(rf,feature_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ede411e",
   "metadata": {},
   "source": [
    "## 13.5 Validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc3eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['Test RMSE', 'Test R$^2$', 'Test MAE']\n",
    "rows=['Regressor Tree', 'Bagged trees', 'Random forest']\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "methods=[tree, bag, rf]\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    \n",
    "     y_pred = method.predict(x_sub_valid)\n",
    "    \n",
    "     results.iloc[i,0] = np.sqrt(mean_squared_error(y_sub_valid, y_pred))\n",
    "     results.iloc[i,1] = r2_score(y_sub_valid, y_pred)\n",
    "     results.iloc[i,2] = mean_absolute_error(y_sub_valid, y_pred)\n",
    "   \n",
    "\n",
    "results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee1a3b2",
   "metadata": {},
   "source": [
    "# 14.0 Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03dda72",
   "metadata": {},
   "source": [
    "## 14.1Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3431c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(learning_rate = 0.01, max_depth = 8, n_estimators= 1901, subsample = 0.596)\n",
    "gb.fit(x_sub_train, y_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4810a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(gb, feature_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(learning_rate = 0.01, max_depth = 8, max_iter = 1901)\n",
    "hgb.fit(x_sub_train, y_sub_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531504d8",
   "metadata": {},
   "source": [
    "## 14.2 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xgbst = xgb.XGBRegressor(objective='reg:squarederror', booster = 'gbtree', learning_rate = 0.02, \n",
    "                        max_depth = 6, n_estimators = 1822, subsample = 0.60,  reg_lambda=0.0, \n",
    "                        reg_alpha = 0.0, colsample_bytree = 1.0, min_child_weight = 1.0)\n",
    "\n",
    "# n_estimators: number of trees\n",
    "# subsample: fraction of observations used to fit each tree\n",
    "# reg_lambda: L2 regularisation penalty\n",
    "# reg_alpha: L1 regularisation penalty\n",
    "# colsample_bytree: fraction of features considered for each tree (alternatives: bylevel, bynode)\n",
    "# min_child_weight: approximately equal to the minimum number of samples in each node\n",
    "\n",
    "xgbst.fit(x_sub_train, y_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(xgbst, labels=feature_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd546ec",
   "metadata": {},
   "source": [
    "## 14.3 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113064b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgbm = lgb.LGBMRegressor(objective='regression', boosting_type = 'gbdt', \n",
    "                         n_estimators = 1974, learning_rate = 0.01,  \n",
    "                         num_leaves = 24,\n",
    "                         lambda_l1 = 0.0, lambda_l2 = 0.0,\n",
    "                         bagging_fraction = 0.74, bagging_freq = 1, \n",
    "                         feature_fraction = 1.0, min_data_in_leaf = 1)\n",
    "\n",
    "lgbm = lgbm.fit(x_sub_train, y_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53688fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(lgbm, labels=feature_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f64eb",
   "metadata": {},
   "source": [
    "### Further LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b0599",
   "metadata": {},
   "source": [
    "### 14.3.1 Training API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5b2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(x_sub_train, y_sub_train)\n",
    "valid_data = lgb.Dataset(x_sub_valid, y_sub_valid, reference = train_data)\n",
    "\n",
    "# An important detail that is not applicable to this tutorial is that LightGBM can handle\n",
    "# categorical features in a specialised way without the need for prior encoding.\n",
    "\n",
    "# Suppose that that the first two columns of X_train are categorical. You'd then run:\n",
    "# train_data = lgb.Dataset(X_train, y_train, categorical_feature = [0, 1])\n",
    "\n",
    "# Alternatively, you can pass the list of categorical features to the train function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'l2',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 31,\n",
    "    'lambda_l1': 0.0, \n",
    "    'lambda_l2': 0.0, \n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_freq': 5,\n",
    "    'feature_fraction': 0.9,\n",
    "    'min_data_in_leaf': 20, \n",
    "}\n",
    "\n",
    "evals = {} \n",
    "lgbm = lgb.train(params, train_data, num_boost_round = 2000, valid_sets = [train_data, valid_data], \n",
    "                 valid_names=['Training', 'Validation'], callbacks = {lgb.record_evaluation(evals)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_metric(evals, metric = 'l2')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc7c991",
   "metadata": {},
   "source": [
    "### 14.3.2 Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 31,\n",
    "    'lambda_l1': 0.0, \n",
    "    'lambda_l2': 0.0, \n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_freq': 5,\n",
    "    'feature_fraction': 0.9,\n",
    "    'min_data_in_leaf': 20\n",
    "}\n",
    "\n",
    "evals = {} \n",
    "\n",
    "lgbm = lgb.train(params, train_data, num_boost_round = 5000,  valid_sets = [train_data, valid_data], \n",
    "                 valid_names=['Training', 'Validation'], callbacks = [lgb.early_stopping(stopping_rounds=100), lgb.record_evaluation(evals)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19556e0f",
   "metadata": {},
   "source": [
    "### 14.3.3 Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_iterations': 5000,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 31,\n",
    "    'lambda_l1': 0.0, \n",
    "    'lambda_l2': 0.0, \n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_freq': 5,\n",
    "    'feature_fraction': 0.9,\n",
    "    'min_data_in_leaf': 20, \n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "\n",
    "history = lgb.cv(params, train_data, num_boost_round = 5000, \n",
    "                 nfold = 5, stratified = False, callbacks = [lgb.early_stopping(stopping_rounds=100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "plt.plot(history['valid l2-mean'])\n",
    "\n",
    "ax.set_ylabel('Cross-validation MSE')\n",
    "ax.set_xlabel('Boosting iterations')\n",
    "\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece946ee",
   "metadata": {},
   "source": [
    "### 14.3.4 Hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8246b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to reconstruct the dataset to tune min_data_in_leaf\n",
    "train_data = lgb.Dataset(x_sub_train, y_sub_train)\n",
    "valid_data = lgb.Dataset(x_sub_valid, y_sub_valid, reference = train_data)\n",
    "\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    params = {\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.01, \n",
    "    'num_leaves': trial.suggest_int('num_leaves', 2, 64),\n",
    "    'lambda_l1': trial.suggest_loguniform('lambda_l1',  1e-8, 10), \n",
    "    'lambda_l2': trial.suggest_loguniform('lambda_l2',  1e-8, 10), \n",
    "    'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n",
    "    'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "    'feature_fraction':  trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n",
    "    'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 128), \n",
    "    'feature_pre_filter': False,\n",
    "    'verbosity': -1,\n",
    "     }\n",
    "    \n",
    "    # Cross-validation \n",
    "    history = lgb.cv(params, train_data, num_boost_round = 5000, \n",
    "                 nfold = 5, stratified = False, callbacks = [lgb.early_stopping(stopping_rounds = 50)])\n",
    "    \n",
    "    # Save full set of parameters\n",
    "    trial.set_user_attr('params', params)\n",
    "    \n",
    "    # Save the number of boosting iterations selected by early stopping\n",
    "    trial.set_user_attr('num_boost_round', len(history['valid l2-mean']))\n",
    "    \n",
    "    return history['valid l2-mean'][-1] # returns CV error for the best trial\n",
    "\n",
    "sampler = TPESampler(seed = 42) \n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=500, timeout = 3000)  \n",
    "\n",
    "# Important! The higher the timeout, the better the chances of finding a good configuration. \n",
    "# The time budget should be much higher in a real application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc736a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = study.best_trial.user_attrs['params']\n",
    "num_trees = study.best_trial.user_attrs['num_boost_round']\n",
    "\n",
    "print(f'Number of boosting iterations: {num_trees} \\n')\n",
    "\n",
    "print('Best parameters:')\n",
    "params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = optuna.importance.get_param_importances(study)\n",
    "\n",
    "importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145db9ca",
   "metadata": {},
   "source": [
    "### 14.3.5 Optuna integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeaf83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip running this cell if you want to save time\n",
    "\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    params = {\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': trial.suggest_int('num_leaves', 2, 64),\n",
    "    'lambda_l1': trial.suggest_loguniform('lambda_l1',  1e-8, 10.0), \n",
    "    'lambda_l2': trial.suggest_loguniform('lambda_l2',  1e-8, 10.0), \n",
    "    'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n",
    "    'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "    'feature_fraction':  trial.suggest_uniform('feature_fraction', 0.3, 1.0),\n",
    "    'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 128),\n",
    "    'feature_pre_filter': False,\n",
    "    'verbosity': -1,\n",
    "     }\n",
    "    \n",
    "    # Instantiate pruning callback\n",
    "    pruning_callback =  LightGBMPruningCallback(trial, 'l2')\n",
    "    \n",
    "    # LightGBM cross-validation\n",
    "    history = lgb.cv(params, train_data, num_boost_round = 5000, \n",
    "                 nfold = 5, stratified = False, \n",
    "                 callbacks = [lgb.early_stopping(stopping_rounds = 50), pruning_callback])\n",
    "    \n",
    "    # Save full set of parameters\n",
    "    trial.set_user_attr('params', params)\n",
    "    \n",
    "    # Save the number of boosting iterations selected by early stopping\n",
    "    trial.set_user_attr('num_boost_round', len(history['valid l2-mean']))\n",
    "    \n",
    "    return history['valid l2-mean'][-1] # returns CV error for the best trial\n",
    "\n",
    "# Instantiating the sampler to set the random seed\n",
    "sampler = TPESampler(seed=42) \n",
    "\n",
    "# Instantiating the pruner\n",
    "# The min_resource should be the minimum number of trees required to reasonably judge performance\n",
    "# This can be tricky to get right for boosting\n",
    "# See the notes in the documentation:\n",
    "# https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.HyperbandPruner.html\n",
    "pruner = optuna.pruners.HyperbandPruner(min_resource = 100, max_resource = 'auto', reduction_factor = 3)\n",
    "\n",
    "# A simpler alternative is the median pruner\n",
    "# This method prunes trials with partial performance below the median of previous trials \n",
    "#pruner = optuna.pruners.MedianPruner(n_startup_trials = 10, n_warmup_steps = 500, interval_steps = 5)\n",
    "\n",
    "study = optuna.create_study(direction='minimize', sampler = sampler, pruner = pruner)\n",
    "study.optimize(objective, n_trials = 500, timeout= 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = study.best_trial.user_attrs['params']\n",
    "num_trees = study.best_trial.user_attrs['num_boost_round']\n",
    "\n",
    "print(f'Number of boosting iterations: {num_trees} \\n')\n",
    "\n",
    "print('Best parameters:')\n",
    "params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14818440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip running this cell if you want to save time\n",
    "\n",
    "import optuna.integration.lightgbm as lgb\n",
    "\n",
    "dtrain = lgb.Dataset(x_sub_train, y_sub_train)\n",
    "    \n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_pre_filter': False,\n",
    "    'verbosity': -1,\n",
    "}\n",
    "\n",
    "tuner = lgb.LightGBMTunerCV(params, dtrain, num_boost_round = 5000, callbacks = [lgb.early_stopping(stopping_rounds = 50)], \n",
    "                            time_budget = 3000, nfold=5, stratified = False)\n",
    "\n",
    "tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d90f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best params:')\n",
    "\n",
    "tuner.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f57314",
   "metadata": {},
   "source": [
    "### 14.3.6 Final LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722193c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {\n",
    " 'objective': 'regression',\n",
    " 'boosting_type': 'gbdt',\n",
    " 'learning_rate': 0.01,\n",
    " 'num_leaves': 45,\n",
    " 'lambda_l1': 0.027109795180157614,\n",
    " 'lambda_l2': 1.3211122996540583e-06,\n",
    " 'bagging_fraction': 0.9303390497096511,\n",
    " 'bagging_freq': 8,\n",
    " 'feature_fraction': 0.6954068017048125,\n",
    " 'min_data_in_leaf': 2}\n",
    "\n",
    "lgbm = lgb.train(params, train_data, num_boost_round = 3996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec92275",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lgb.LGBMRegressor(objective= 'regression',\n",
    " metric= 'l2',\n",
    " boosting_type= 'gbdt',\n",
    " learning_rate= 0.05,\n",
    " feature_pre_filter= False,\n",
    " verbosity= -1,\n",
    " lambda_l1= 2.580935030198812e-06,\n",
    " lambda_l2= 3.4697812081022227e-06,\n",
    " num_leaves= 84,\n",
    " feature_fraction= 0.4,\n",
    " bagging_fraction= 1.0,\n",
    " bagging_freq= 0,\n",
    " min_child_samples= 20)\n",
    "\n",
    "lgbm = lgbm.fit(x_sub_train, y_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(lgbm, labels=feature_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c747585",
   "metadata": {},
   "source": [
    "## 14.4 CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cb = CatBoostRegressor(loss_function='RMSE', \n",
    "                       iterations = 2000, learning_rate = 0.01,\n",
    "                       l2_leaf_reg = 0.0,\n",
    "                       max_depth = 6, \n",
    "                       subsample = 0.8, \n",
    "                       verbose=0)\n",
    "        \n",
    "cb.fit(x_sub_train, y_sub_train)\n",
    "\n",
    "# Important! If there were categorical features, you'd want to change the code to\n",
    "# cb.fit(X_train, y_train, cat_features = cat_indices)\n",
    "# where cat_indices is a one-dimensional array of indices indicating the categorical columns in X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8babd58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = CatBoostRegressor(loss_function='RMSE', verbose=0)        \n",
    "cb.fit(x_sub_train, y_sub_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15433f67",
   "metadata": {},
   "source": [
    "## 14.5 Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary code - important to understand but similar to previous tutorials\n",
    "\n",
    "columns=['RMSE', 'R$^2$', 'MAE']\n",
    "rows=[ 'GBM', 'Histogram-based GBM',\n",
    "      'XGBoost', 'CatBoost', 'LightGBM']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "methods = [gb, hgb, xgbst, cb, lgbm]\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "\n",
    "    y_pred = method.predict(x_sub_valid)\n",
    "    \n",
    "    results.iloc[i,0] = np.sqrt(mean_squared_error(y_sub_valid, y_pred))\n",
    "    results.iloc[i,1] = r2_score(y_sub_valid, y_pred)\n",
    "    results.iloc[i,2] = mean_absolute_error(y_sub_valid, y_pred)\n",
    "\n",
    "results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77867a6d",
   "metadata": {},
   "source": [
    "# 15.0 Model stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d3263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    " \n",
    "stack = StackingRegressor(estimators = [('LightGBM', lgbm),\n",
    "                                        ('Random forest', rf),('Catboost', cb)], final_estimator = LinearRegression(), \n",
    "                           cv = 5)\n",
    "\n",
    "stack.fit(x_sub_train, y_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1964610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.final_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9dc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise table\n",
    "columns=['RMSE', 'R-Squared', 'MAE']\n",
    "rows=[ 'LightGBM', 'Random forest','Catboost' ,'Stack']\n",
    "\n",
    "results =pd.DataFrame(0.0, columns=columns, index=rows)\n",
    "\n",
    "# Methods\n",
    "methods = [lgbm,rf,cb,stack]\n",
    "\n",
    "# Compute test predictions\n",
    "y_pred = np.zeros((len(y_sub_valid), 4))\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    \n",
    "    y_pred[:, i] = method.predict(x_sub_valid)\n",
    "    \n",
    "# Validation results\n",
    "for i in range(len(rows)):\n",
    "    results.iloc[i, 0] = np.sqrt(mean_squared_error(y_sub_valid, y_pred[:, i]))\n",
    "    results.iloc[i, 1] = r2_score(y_sub_valid, y_pred[:, i])\n",
    "    results.iloc[i, 2] = mean_absolute_error(y_sub_valid, y_pred[:, i])\n",
    "\n",
    "results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a6292",
   "metadata": {},
   "source": [
    "# 16.0 Model assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded5100",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d040a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['LogPrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test.drop(columns=['LogPrice','price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.exp(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafdd6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train = pd.concat([x_sub_train, x_sub_valid], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f610a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Train = pd.concat([y_sub_train, y_sub_valid], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.fit(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893abb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['RMSE', 'R-Squared', 'MAE']\n",
    "rows=['Stack']\n",
    "\n",
    "results =pd.DataFrame(0.0, columns=columns, index=rows)\n",
    "\n",
    "# Methods\n",
    "methods = [stack]\n",
    "\n",
    "# Compute test predictions\n",
    "y_pred = np.zeros((len(y_test), 1))\n",
    "                  \n",
    "for i, method in enumerate(methods):\n",
    "    \n",
    "    y_pred[:, i] = method.predict(x_test)\n",
    "    \n",
    "# Validation results\n",
    "for i in range(len(rows)):\n",
    "    results.iloc[i, 0] = np.sqrt(mean_squared_error(y_test, y_pred[:, i]))\n",
    "    results.iloc[i, 1] = r2_score(y_test, y_pred[:, i])\n",
    "    results.iloc[i, 2] = mean_absolute_error(y_test, y_pred[:, i])\n",
    "\n",
    "results.round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
